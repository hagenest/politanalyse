{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "You need to have the following packages installed and you have to download the spacy model for German, if you want to do the tokenization yourself.\n",
    "    \n",
    "        pip install spacy\n",
    "        python -m spacy download de_core_news_lg\n",
    "\n",
    "Dataset-Citation:\n",
    " Richter, Florian; Koch, Philipp; Franke, Oliver; Kraus, Jakob; Kuruc, Fabrizio; Thiem, Anja; Högerl, Judith; Heine, Stella; Schöps, Konstantin, 2020, \"Open Discourse\", https://doi.org/10.7910/DVN/FIKIBO, Harvard Dataverse, V3; speeches.csv [fileName]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you have to run this\n",
    "\n",
    "import pandas as pd\n",
    "import plotly\n",
    "import spacy\n",
    "from wordcloud import WordCloud, get_single_color_func\n",
    "from pathlib import Path\n",
    "import requests\n",
    "\n",
    "data_path = Path(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This block and the next two are only needed if you want to tokenize the speeches yourself, which takes like an hour.\n",
    "# If you want to use the pre-tokenized version, skip this block and the next two.\n",
    "\n",
    "nlp = spacy.load('de_core_news_lg')\n",
    "if not Path(data_path / 'speeches.csv').is_file():\n",
    "    url = 'https://dataverse.harvard.edu/api/access/datafile/4745985'\n",
    "    r = requests.get(url)\n",
    "    with open('speeches.csv', 'wb') as f:\n",
    "        f.write(r.content)\n",
    "    \n",
    "SPEECHES = pd.read_csv(Path(data_path / 'speeches.csv'), index_col=0)\n",
    "SPEECHES.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "electoral_terms = [18, 19]\n",
    "current_speeches = SPEECHES[SPEECHES['electoralTerm'].isin(electoral_terms)]\n",
    "\n",
    "factions = {\n",
    "    'AFD': 0,\n",
    "    'Grüne': 3,\n",
    "    'Union': 4,\n",
    "    'Linke': 6,\n",
    "    'FDP': 13,\n",
    "    'SPD': 23,\n",
    "}\n",
    "\n",
    "speeches = []\n",
    "\n",
    "# warning this takes at least 40 minutes, after that I went to bed\n",
    "\n",
    "for faction in factions.values():\n",
    "    print(faction)\n",
    "    factions_speeches = current_speeches[current_speeches['factionId'] == faction]\n",
    "    speech_content = factions_speeches['speechContent'].to_list()\n",
    "    speech_content = [str(speech) for speech in speech_content] # seems sadly necessary\n",
    "    speech_nlp = [nlp(speech) for speech in speech_content]\n",
    "    words = [[token for token in speech if token.is_stop != True and token.pos_ == \"NOUN\"] for speech in speech_nlp]\n",
    "    words_str = [' '.join([token.text for token in speech]) for speech in words]\n",
    "    speeches.append(words_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_frequency(long_str) -> pd.DataFrame:\n",
    "    \"\"\"Returns a dataframe with the word frequency of a string\"\"\"\n",
    "    words = []\n",
    "    for entry in long_str:\n",
    "        words.extend(entry.split())\n",
    "    word_freq = {}\n",
    "    for word in words:\n",
    "        if word in word_freq:\n",
    "            word_freq[word] += 1\n",
    "        else:\n",
    "            word_freq[word] = 1\n",
    "    # three columns, index, word, and frequency\n",
    "    word_freq = pd.DataFrame.from_dict(word_freq, orient='index', columns=['frequency'])\n",
    "    word_freq = word_freq.sort_values(by='frequency', ascending=False)\n",
    "    return word_freq\n",
    "\n",
    "afd_freq = word_frequency(speeches[0]).sort_values(by='frequency', ascending=False)\n",
    "gruene_freq = word_frequency(speeches[1]).sort_values(by='frequency', ascending=False)\n",
    "union_freq = word_frequency(speeches[2]).sort_values(by='frequency', ascending=False)\n",
    "linke_freq = word_frequency(speeches[3]).sort_values(by='frequency', ascending=False)\n",
    "fdp_freq = word_frequency(speeches[4]).sort_values(by='frequency', ascending=False)\n",
    "spd_freq = word_frequency(speeches[5]).sort_values(by='frequency', ascending=False)\n",
    "\n",
    "afd_freq.to_csv(Path(data_path / 'afd_freq.csv'), sep='#')\n",
    "gruene_freq.to_csv(Path(data_path / 'gruene_freq.csv'), sep='#')\n",
    "union_freq.to_csv(Path(data_path / 'union_freq.csv'), sep='#')\n",
    "linke_freq.to_csv(Path(data_path / 'linke_freq.csv'), sep='#')\n",
    "fdp_freq.to_csv(Path(data_path / 'fdp_freq.csv'), sep='#')\n",
    "spd_freq.to_csv(Path(data_path / 'spd_freq.csv'), sep='#')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# continue here if you want to use the pre-tokenized version\n",
    "\n",
    "afd_freq = pd.read_csv(Path(data_path / 'afd_freq.csv'), sep='#', index_col=0)\n",
    "gruene_freq = pd.read_csv(Path(data_path / 'gruene_freq.csv'), sep='#', index_col=0)\n",
    "union_freq = pd.read_csv(Path(data_path / 'union_freq.csv'), sep='#', index_col=0)\n",
    "linke_freq = pd.read_csv(Path(data_path / 'linke_freq.csv'), sep='#', index_col=0)\n",
    "fdp_freq = pd.read_csv(Path(data_path / 'fdp_freq.csv'), sep='#', index_col=0)\n",
    "spd_freq = pd.read_csv(Path(data_path / 'spd_freq.csv'), sep='#', index_col=0)\n",
    "\n",
    "freqs = [afd_freq, gruene_freq, union_freq, linke_freq, fdp_freq, spd_freq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_frequency(df_list):\n",
    "    \"\"\"\n",
    "    Normalizes the frequency of a list of dataframes for a faction, so that characterisitcs of a faction can be compared.\n",
    "    (Frequency by one faction / total words of one faction) / (Frequency by all factions / total words of all factions)\n",
    "    I'm sure this could've been done in a less convoluted way, PRs are welcome\n",
    "    \"\"\"\n",
    "    total_words = [sum(df['frequency']) for df in df_list]\n",
    "    total_words_all = sum(total_words)\n",
    "    normalized = pd.DataFrame()\n",
    "    afd_norm = []\n",
    "    gruene_norm = []\n",
    "    union_norm = []\n",
    "    linke_norm = []\n",
    "    fdp_norm = []\n",
    "    spd_norm = []\n",
    "    x = 5\n",
    "    for word in df_list[0].index:\n",
    "        freq = [df.loc[word]['frequency'] if word in df.index else 0 for df in df_list]\n",
    "        afd_norm.append((freq[0] + x) / total_words[0] / ((sum(freq) + x) / total_words_all))\n",
    "        gruene_norm.append(freq[1] / total_words[1] / ((sum(freq) + x) / total_words_all))\n",
    "        union_norm.append(freq[2] / total_words[2] / ((sum(freq) + x) / total_words_all))\n",
    "        linke_norm.append(freq[3] / total_words[3] / ((sum(freq) + x) / total_words_all))\n",
    "        fdp_norm.append(freq[4] / total_words[4] / ((sum(freq) + x) / total_words_all))\n",
    "        spd_norm.append(freq[5] / total_words[5] / ((sum(freq) + x) / total_words_all))\n",
    "    normalized['AFD'] = afd_norm\n",
    "    normalized['Grüne'] = gruene_norm\n",
    "    normalized['Union'] = union_norm\n",
    "    normalized['Linke'] = linke_norm\n",
    "    normalized['FDP'] = fdp_norm\n",
    "    normalized['SPD'] = spd_norm\n",
    "    normalized.index = df_list[0].index\n",
    "    return normalized\n",
    "\n",
    "\n",
    "normalized = normalize_frequency([afd_freq, gruene_freq, union_freq, linke_freq, fdp_freq, spd_freq])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_path = Path('visualizations')\n",
    "\n",
    "colors = {\n",
    "    'AFD' : get_single_color_func('SaddleBrown'),\n",
    "    'Grüne' : get_single_color_func('Green'),\n",
    "    'Union' : get_single_color_func('darkblue'),\n",
    "    'Linke' : get_single_color_func('MediumVioletRed'),\n",
    "    'FDP' : get_single_color_func('Gold'),\n",
    "    'SPD' : get_single_color_func('red')\n",
    "}\n",
    "\n",
    "def word_clouds(df) -> None:\n",
    "    \"\"\"\n",
    "    Creates a word cloud for each faction in a dataframe\n",
    "    \"\"\"\n",
    "    for faction in factions.keys():\n",
    "        print(faction)\n",
    "        # don't write words vertically\n",
    "        wordcloud = WordCloud(width=800, height=400, background_color='white', color_func=colors[faction], prefer_horizontal=1).generate_from_frequencies(df[faction])\n",
    "        wordcloud.to_file(viz_path / f'{faction}.png')\n",
    "\n",
    "word_clouds(normalized)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
